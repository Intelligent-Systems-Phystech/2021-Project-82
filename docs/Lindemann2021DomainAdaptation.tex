\documentclass[12pt, twoside]{article}
\usepackage{jmlda}
\newcommand{\hdir}{.}

\newtheorem{theorem}{Теорема}
\newtheorem{lemma}[theorem]{Лемма}
\newtheorem{definition}{Определение}

\DeclareMathOperator{\Acc}{Accuracy}
\DeclareMathOperator{\Precision}{Precision}
\DeclareMathOperator{\Recall}{Recall}


\begin{document}

\title
    {Обучение экспертов для задачи прогнозирования со многими доменами} % Название
\author
    {Н.\,А.~Линдеманн, А.\,В.~Грабовой} % основной список авторов, выводимый в оглавление
\email
    {lindemann.na@phystech.edu; andriy.graboviy@mail.ru}
%\thanks
%    {Работа выполнена при
%     %частичной
%     финансовой поддержке РФФИ, проекты \No\ \No 00-00-00000 и 00-00-00001.}
%\organization
%    {$^1$Организация, адрес; $^2$Организация, адрес}
\abstract
    {Рассматривается задача аппроксимации выборки со многими доменами единой мультимоделью -- смесью экспертов. Каждый домен аппроксимируется локальной моделью. В работе рассматривается двухэтапная задача оптимизации на основе ЕМ-алгоритма. Используется выборка отзывов сайта Amazon для разных типов товара, которая содержит в себе несколько доменов. В качестве эксперта используется линейная модель, а в качестве признакового описания отзывов используются tf-idf вектора внутри каждого домена.

	
\bigskip
\noindent
\textbf{Ключевые слова}: \emph {Смесь экспертов, обучение экспертов, классификация текстов.}

}

%данные поля заполняются редакцией журнала
%\doi{10.21469/22233792}
%\receivedRus{01.01.2017}
%\receivedEng{January 01, 2017}

\maketitle
\linenumbers

\section{Введение}
На текущий момент в машинном обучении появляется все больше задач связанных с данными, которые взяты с разных источников. Часто появляются выборки, которые состоят из большого числа доменов. Под \textit{доменом} понимается подмножество объектов выборки, которые обладают некоторыми одинаковыми признаками. 

В работе рассматривается задача аппроксимации выборки со многими доменами смесью экспертов. Рассматривается задача бинарной классификации текстов, используется дополнительная информация о доменах. Прдполагается, что это позволит строить более простые и интерпретируемые модели.

Метод решения задачи состоит в построении мультимодели, являющейся смесью локальных моделей. Каждый домен аппроксимируется локальной линейной моделью, смесь которых является итоговым классификатором. Задача обучения модели сводится к двухэтапной задаче оптимизации на основе ЕМ-алгоритма. Схема способа построения смеси экспертов представлена на рис.~\ref{example:1}.

\begin{figure}[H]\center
\includegraphics[width=0.5\textwidth]{fig/Schem_1.jpg}
\caption{Способ построения смеси экспертов.}
\label{example:1}
\end{figure}

Используется выборка отзывов сайта Amazon для разных типов товаров, которая содержит в себе несколько доменов. Каждый объект имеет экспертно-зависимое описание, которое определяется его пренадлежностью к тому или иному домену. В качестве признакового описания отзывов используется tf-idf вектора внутри каждого домена. 

\section{Постановка задачи}
\subsection{Постановка задачи обучнения одного эксперта}
Задача бинарной классификации является задачей апроксимации целевой функции
\[
\label{eq:st:1.1}
\begin{aligned}
\mathbf{f}: \mathbb{R}^n \to \{-1, +1\},
\end{aligned}
\]
где $\mathbb{R}^n$ -- пространство признакового описания объектов, а $\{-1, +1\}$ -- метка класса объекта. Задачей локальной модели является апроксимация функции $\mathbf{f}$ на некотором домене. На основе общих признаков $(x_1, \ldots, x_n)$ эксперт генерирует экспертно-зависимые признаки $(x_{n+1}, \ldots, x_{m_k})$, количество которых зависит от конкретного домена, и с помощью признаков $(x_1, \ldots, x_n, x_{n+1}, \ldots, x_{m_k})$ локальная модель делает предсказание о принадлежности объекта к одному из двух классов.

В качестве локальной модели будем использовать логистическую регрессию, которая будет предсказывать вероятность того, что объект с признаковым описанием $\vec{x_i}$ принадлежит классу $y_i$:
\[p\left(y = y_i \mid \vec{x_i}, \mathbf{w}\right) = \sigma(y_i \mathbf{w} \cdot \vec{x_i}).\]

Рассмотрим правдоподобие выборки, а именно, вероятность наблюдать данный вектор $\vec{y}$ у домена $\textbf{C}$ (выборка размера $N$). В предположении, что объекты выборки внутри одного домена независимы и из одного распределения, получим:
\[p \left(\vec{y} \mid \textbf{C}, \mathbf{w}\right) = \prod_{i=1}^{N} p\left(y = y_i \mid \vec{x_i}, \mathbf{w}\right).\]
Далее рассмотрим логарифм правдоподобия:
\[\log p\left(\vec{y} \mid \textbf{C}, \mathbf{w}\right) = \log \prod_{i=1}^{N} \sigma(y_i \mathbf{w} \cdot \vec{x_i}) = \sum_{i=1}^{N} \log \frac{1}{1 + e^{-y_i \mathbf{w} \cdot \vec{x_i}}} = - \sum_{i=1}^{N} \log (1 + e^{-y_i \mathbf{w} \cdot \vec{x_i}}).\]
Значит, в даном случае принцип максимального правдоподобия приводит к минимизации логистической функции потерь по всем объектам из данного домена:
\[\mathcal{L} (\textbf{C}, \vec{y}, \mathbf{w}) = \sum_{i=1}^{N} \log (1 + e^{-y_i \mathbf{w} \cdot \vec{x_i}}) \to \min_{\mathbf{w}}.\]



\subsection{Постановка задачи построения смеси экспертов}
Обобщим подход аппроксимации одного домена на случай, когда в данных присутствует несколько доменов. Пусть всего имеется~$K$ доменов в выборке, тогда всю выборку $\textbf{C}$ можно представить в виде:
\[
\label{eq:st:1}
\begin{aligned}
\textbf{C} = \bigsqcup \limits_{k=1}^{K}\textbf{C}_{k}',
\end{aligned}
\]
где~$\textbf{C}_{k}'$ множество объектов, принадлежащих $k$-му домену. Множеству объектов из домена~$\textbf{C}_{k}' \subset\textbf{C}$ соответствует задача линейной регрессии для выборки~$\textbf{X}_{k}' \subset \textbf{X}, \textbf{y}_{k}' \subset \textbf{y}$. Модель~$\mathbf{g}_k$ аппроксимирующая выборку~$\textbf{X}_{k}', \textbf{y}_{k}'$ является локальной моделью для выборки \textbf{X}, \textbf{y}.


\begin{definition}
\label{def:1}
Модель~$\mathbf{g}$ называется локальной моделью для выборки~$\textbf{U},$ если~$\mathbf{g}$ аппроксимирует некоторое не пустое подмножество~$\textbf{U}'\subset\textbf{U}$.
\end{definition}

\begin{definition}
\label{def:2}
Мультимодель~$\mathbf{f}$ называется смесью экспертов, если:
\[
\label{eq:st:2}
\begin{aligned}
\mathbf{f} = \sum_{k=1}^{K}\pi_{k}\mathbf{g}_k\bigr(\mathbf{w}_k\bigr), \qquad \pi_{k}\bigr(\mathbf{x}, \mathbf{V}\bigr):\mathbb{R}^{n\times \left|\mathbf{V}\right|} \to [0, 1], \qquad \sum_{k=1}^{K}\pi_{k}\bigr(\mathbf{x}, \mathbf{V}\bigr) = 1,
\end{aligned}
\]
где~$\mathbf{g}_k$ является~$k$-й локальной моделью,~$\pi_k$ --- шлюзовая функция, вектор~$\mathbf{w}_k$ является параметрами~$k$-й локальной моделью, а~$\mathbf{V}$ --- параметры шлюзовой функции.
\end{definition}

Пусть~$\mathbf{w}_k$ является случайным вектором, который задается плотностью распределения~$p^{k}\bigr(\mathbf{w}_k\bigr)$. Получим совместное распределения параметров локальных моделей и вектора ответов:
\[
\label{eq:st:3}
\begin{aligned}
p\bigr(\mathbf{y}, \mathbf{W}|\mathbf{X}, \mathbf{V}\bigr) = \prod_{i=1}^{N}\left(\sum_{k=1}^{K}\pi_{k}p_{k}\bigr(y_i|\mathbf{w}_k, \mathbf{x}_i\bigr)\right),
\end{aligned}
\]
где~$\mathbf{W} = \bigr\{\mathbf{w}_1, \mathbf{w}_2, \cdots, \mathbf{w}_K\bigr\}.$
Оптимальные параметры находятся при помощи максимизации правдоподобия:
\[
\label{eq:st:4}
\begin{aligned}
\hat{\mathbf{V}}, \hat{ \mathbf{W}} = \arg\max_{\mathbf{V}, \mathbf{W}} p\bigr(\mathbf{y},  \mathbf{W}|\mathbf{X}, \mathbf{V}\bigr).
\end{aligned}
\]

\section{Вычислительный эксперимент}
\subsection{Анализ данных}
Для проведения первого вычислительного эксперимента из всех отзывов с сайта Amazon были выбраны пять разных доменов: music, baby, kitchen\_\&\_housewares, software, books. Выбор именно этих доменов был  обусловлен тем, что получившаяся разнородная подвыборка содержала $9815$ и была гиперсбалансирована:
\begin{figure}[h]
\begin{center}
\begin{minipage}[h]{0.4\linewidth}
\includegraphics[width=1\linewidth]{fig/classes_dist.pdf}
\caption{Распределение подвыборки по классам.} %% подпись к рисунку
\end{minipage}
\hfill
\begin{minipage}[h]{0.4\linewidth}
\includegraphics[width=1\linewidth]{fig/domens_dist.pdf}
\caption{Распределение подвыборки по доменам.}
\end{minipage}
\end{center}
\end{figure}

Далее полученная выборка была разделена на тестовую и обучающую в пропорции $30:70$. После этого тексты отзывов были преобразованы к формату tf-idf размерности $25770$.

Основными метриками, по которым мы будем оценивать качество работы модели, будут
\[\text{Accuracy} = \frac{\text{tp} + \text{tn}}{\text{tp} + \text{tn} + \text{fp}+ \text{fn}}, \hspace{10pt} \text{Precision} = \frac{\text{tp}}{\text{tp} + \text{fp}}, \hspace{10pt} \text{Recall} = \frac{\text{tp}}{\text{tp} + \text{fn}}.\]


\subsection{Эксперимент с одной моделью}
После подготовки данных была обучена одна модель. Эта модель представляла собой логистическую регрессию с логистической функцией потерь. Результаты обучения модели представлены на рисунках
\begin{figure}[h]
\begin{center}
\begin{minipage}[h]{0.4\linewidth}
\includegraphics[width=1\linewidth]{fig/res.jpg}
\caption{Метрики обученной модели на тестовых данных.} %% подпись к рисунку
\end{minipage}
\hfill
\begin{minipage}[h]{0.4\linewidth}
\includegraphics[width=1\linewidth]{fig/res.jpg}
\caption{Метрики обученной модели на обучающих данных.}
\end{minipage}
\end{center}
\end{figure}
 

\subsection{Эксперимент с мультимоделью}


\section{Анализ задачи}
По условию задачи:

$$p(\textbf{X}, \textbf{y}, \textbf{w}| \textbf{A}) = \prod_i N(\textbf{x}_i|\textbf{0}, \sigma^2 \textbf{I}_n)N(\textbf{w}|\textbf{0}, \textbf{A}^{-1})\prod_j p(y_j| \textbf{x}_j, \textbf{w}) , \eqno(1.1)$$
где $p(y_j = 1|\textbf{x}_j, \textbf{w}) = \frac{1}{1+\exp(-\textbf{w}^{\mathsf{T}}\textbf{x}_j)}$

Для простоты запишем (1.1) в следующем общем виде:
$$p(\textbf{X}, \textbf{y}, \textbf{w}| \textbf{A}) = p(\textbf{X})p(\textbf{w}|\textbf{A})p(\textbf{y}|\textbf{X},\textbf{w}). \eqno(1.2)$$

По формуле Байеса:
$$p(\textbf{w}|\textbf{X},\textbf{y},\textbf{A}) = \frac{p(\textbf{y}|\textbf{X},\textbf{w})p(\textbf{w}|\textbf{A})}{\int_{\textbf{w}'\in \mathbb{R}^{n}} p(\textbf{y}|\textbf{X},\textbf{w}')p(\textbf{w}'|\textbf{A}) d\textbf{w}'}= \frac{\mathcal{Q}(\textbf{w})}{\int_{\textbf{w}'\in \mathbb{R}^{n}} \mathcal{Q}(\textbf{w}')}, \eqno(1.3)$$
где введено обозначение $\mathcal{Q}(\textbf{w}) = p(\textbf{y}|\textbf{X},\textbf{w})p(\textbf{w}|\textbf{A})$.

Выполним аппроксимацию Лапласа:
$$\text{log}\mathcal{Q}(\textbf{w})\approx \text{log}\mathcal{Q}(\textbf{w}_\text{MAP}) + \nabla  \text{log}\mathcal{Q}(\textbf{w}_\text{MAP}) + \frac{1}{2}(\textbf{w}-\textbf{w}_{\text{MAP}})^{\mathsf{T}}\nabla\nabla^{\mathsf{T}}  \text{log}\mathcal{Q}(\textbf{w}_\text{MAP})(\textbf{w}-\textbf{w}_{\text{MAP}})=$$
$$= \text{log}\mathcal{Q}(\textbf{w}_\text{MAP}) - \frac{1}{2}(\textbf{w}-\textbf{w}_{\text{MAP}})^{\mathsf{T}}\textbf{H}^{-1}(\textbf{w}-\textbf{w}_{\text{MAP}}), \eqno(1.4)$$
где введено обозначение $\textbf{H}^{-1} = -\nabla\nabla^{\mathsf{T}}  \text{log}\mathcal{Q}(\textbf{w}_\text{MAP})$.

Для нашей задачи найдем $\textbf{H}^{-1}$:
$$\textbf{H}^{-1} = -\nabla\nabla^{\mathsf{T}}(-\frac{1}{2}\textbf{w}^{\mathsf{T}}\textbf{A}^{-1}\textbf{w} - \textbf{1}^{\mathsf{T}}\log(1+\exp(-\textbf{X}^{\mathsf{T}}\textbf{w}))) =$$
$$= \textbf{A}^{-1} +\nabla\nabla^{\mathsf{T}}\textbf{1}^{\mathsf{T}}\log(1+\exp(-\textbf{X}^{\mathsf{T}}\textbf{w}))=$$
$$=  \textbf{A}^{-1} +\sum_{i=1}^{m} \nabla\nabla^{\mathsf{T}}\log(1+\exp(-\textbf{x}_i^{\mathsf{T}}\textbf{w}))=$$
$$=  \textbf{A}^{-1} +\sum_{i=1}^{m} \textbf{x}_i\textbf{x}_i^{\mathsf{T}}\frac{\exp(-\textbf{x}_i^{\mathsf{T}}\textbf{w})}{1+\exp(-\textbf{x}_i^{\mathsf{T}}\textbf{w})}-\textbf{x}_i\textbf{x}_i^{\mathsf{T}}\frac{\exp(-2\textbf{x}_i^{\mathsf{T}}\textbf{w})}{(1+\exp(-\textbf{x}_i^{\mathsf{T}}\textbf{w}))^2}. \eqno(1.5)$$

Тогда получаем:
$$\mathcal{Q}(\textbf{w}) \approx \mathcal{Q}(\textbf{w}_{\text{MAP}})\exp\left(-\frac{1}{2}(\textbf{w}-\textbf{w}_{\text{MAP}})^{\mathsf{T}}\textbf{H}^{-1}(\textbf{w}-\textbf{w}_{\text{MAP}})\right). \eqno(1.6)$$

Подставляя (1.6) в (1.3) получим:

$$p(\textbf{w}|\textbf{X},\textbf{y},\textbf{A}) \approx \frac{\mathcal{Q}(\textbf{w}_{\text{MAP}})\exp\left(-\frac{1}{2}(\textbf{w}-\textbf{w}_{\text{MAP}})^{\mathsf{T}}\textbf{H}^{-1}(\textbf{w}-\textbf{w}_{\text{MAP}})\right)}{\int_{\textbf{w}'\in \mathbb{R}^{n}} \mathcal{Q}(\textbf{w}_{\text{MAP}})\exp\left(-\frac{1}{2}(\textbf{w}'-\textbf{w}_{\text{MAP}})^{\mathsf{T}}\textbf{H}^{-1}(\textbf{w}'-\textbf{w}_{\text{MAP}})\right)d\textbf{w}'}=$$
$$= \frac{\exp\left(-\frac{1}{2}(\textbf{w}-\textbf{w}_{\text{MAP}})^{\mathsf{T}}\textbf{H}^{-1}(\textbf{w}-\textbf{w}_{\text{MAP}})\right)}{\int_{\textbf{w}'\in \mathbb{R}^{n}} \exp\left(-\frac{1}{2}(\textbf{w}'-\textbf{w}_{\text{MAP}})^{\mathsf{T}}\textbf{H}^{-1}(\textbf{w}'-\textbf{w}_{\text{MAP}})\right)d\textbf{w}'} = $$
$$ = N(\textbf{w}_{\text{MAP}}, \textbf{H}).\eqno(1.7)$$

Оценим $\textbf{w}_{\text{MAP}}$:
$$\textbf{w}_{\text{MAP}} = \argmax_{\textbf{w} \in \mathbb{R}^{n}} p(\textbf{w}|\textbf{X}, \textbf{y}, \textbf{A}) = \argmin_{\textbf{w} \in \mathbb{R}^{n}}\{-\log p(\textbf{y}|\textbf{X},\textbf{w}) - \log p(\textbf{w}|\textbf{A})\}, \eqno(1.8)$$
где $$p(\textbf{y}|\textbf{X}, \textbf{w}) = \prod_i \hat{p}_i^{y_i}(1-\hat{p}_i)^{1-y_i}; \quad -\log p(\textbf{w}|\textbf{A}) = \frac{1}{2}\textbf{w}^{\mathsf{T}}\textbf{A}^{-1}\textbf{w}; \quad\hat{\textbf{p}} = \frac{1}{1+\exp(-\textbf{X}^{\mathsf{T}}\textbf{w})}, \eqno(1.9)$$

Подставляя (1.9) в (1.8) получаем:
$$\textbf{w}_{\text{MAP}} = \argmin_{\textbf{w} \in \mathbb{R}^{n}}\{-\textbf{y}^{\mathsf{T}}\log \hat{\textbf{p}} - (1-\textbf{y})^{\mathsf{T}}\log (1-\hat{\textbf{p}}) +\frac{1}{2}\textbf{w}^{\mathsf{T}}\textbf{A}^{-1}\textbf{w}\}=$$
$$=\argmin_{\textbf{w} \in \mathbb{R}^{n}}\{
-\textbf{y}^{\mathsf{T}}\log \frac{1}{1+\exp(-\textbf{X}^{\mathsf{T}}\textbf{w})}
-(\textbf{1} - \textbf{y})^{\mathsf{T}}\log \frac{\exp(-\textbf{X}^{\mathsf{T}}\textbf{w})}{1+\exp(-\textbf{X}^{\mathsf{T}}\textbf{w})}
+\frac{1}{2}\textbf{w}^{\mathsf{T}}\textbf{A}^{-1}\textbf{w}\}\} = $$
$$=\argmin_{\textbf{w} \in \mathbb{R}^{n}}\{
(\textbf{1} - \textbf{y})^{\mathsf{T}}\textbf{X}^{\mathsf{T}}\textbf{w}
+\textbf{1}^{\mathsf{T}}\log(1+\exp(-\textbf{X}^{\mathsf{T}}\textbf{w}))
+\frac{1}{2}\textbf{w}^{\mathsf{T}}\textbf{A}^{-1}\textbf{w}\}, \eqno(1.10)$$
где введя обозначения $\mathcal{L}(\textbf{w}|\textbf{X},\textbf{y},\textbf{A}) =
(\textbf{1} - \textbf{y})^{\mathsf{T}}\textbf{X}^{\mathsf{T}}\textbf{w}
+\textbf{1}^{\mathsf{T}}\log(1+\exp(-\textbf{X}^{\mathsf{T}}\textbf{w}))
+\frac{1}{2}\textbf{w}^{\mathsf{T}}\textbf{A}^{-1}\textbf{w}$ получим следующую оптимизационую задачу для нахождения $\textbf{w}_{\text{MAP}}$:
$$\textbf{w}_{\text{MAP}} = \argmin_{\textbf{w} \in \mathbb{R}^{n}} \mathcal{L}(\textbf{w}|\textbf{X},\textbf{y},\textbf{A}). \eqno(1.11)$$
Данная оптимизационная задача решается с помощью градиентного спуска.


\section{Заключение}
На текущий момент не существует полного теоретического обоснования построения смесей локальных моделей для аппроксимации такого рода выборок. 

%%%% если имеется doi цитируемого источника, необходимо его указать, см. пример в \bibitem{article}
%%%% DOI публикации, зарегистрированной в системе Crossref, можно получить по адресу http://www.crossref.org/guestquery/
\begin{thebibliography}{99}


 \bibitem{article1}
    \BibAuthor{J.~Jiang}.
   A Literature Survey on Domain Adaptation of Statistical Classifiers~//
    \BibJournal{?????}, 2007 % http://www.mysmu.edu/faculty/jingjiang/papers/da_survey.pdf
    
 \bibitem{article2}
    \BibAuthor{А.В.~Грабовой, В.В.~Стрижов}.
   Анализ выбора априорного распределения для смеси экспертов~//
    \BibJournal{?????}, 2018 %

 \bibitem{article3}
    \BibAuthor{G.~Wilson, D.J.~Cook}.
   A Survey of Unsupervised Deep Domain Adaptation~//
    \BibJournal{ACM Transactions on Intelligent Systems and Technology}, 2020
    
 \bibitem{article4}
    \BibAuthor{M.~Wang, W.~Deng}.
   Deep Visual Domain Adaptation: A Survey~//
    \BibJournal{Manuscript accepted by Neurocomputing}, 2018
    
 \bibitem{article5}
    \BibAuthor{J.~Guo, D.J.~Shah, R.~Barzilay}.
   Multi-Source Domain Adaptation with Mixture of Experts~//
    \BibJournal{Conference on Empirical Methods in Natural Language Processing}, 2018
 
 	
\end{thebibliography}

%%%% если имеется doi цитируемого источника, необходимо его указать, см. пример в \bibitem{article}
%%%% DOI публикации, зарегистрированной в системе Crossref, можно получить по адресу http://www.crossref.org/guestquery/.

\end{document}

\begin{figure}[H]
\begin{minipage}[h]{0.49\linewidth}
\center{\includegraphics[width=0.5\linewidth]{} \\ а)}
\end{minipage}
\hfill
\begin{minipage}[h]{0.49\linewidth}
\center{\includegraphics[width=0.5\linewidth]{} \\ б)}
\end{minipage}
\caption{Зависимость сигнала от шума для данных.}
\end{figure}